# AI_theory

# Entropy
사디 카르노  
산업혁명 시대 증기기관 발명  
"열은 일을 할 수 있다."  
"열은 일로 전환되어 일을 한 뒤 완전히 보존되어 열로 다시 나온다." (칼로릭 보존)  
"열은 높은 곳에서 낮은 곳으로 이동한다."  

제임스 줄  
아니다. "에너지 보존 법칙에 의해 열은 일로 전환된 만큼 소모되어 줄어든다."  
  
윌리엄 톰슨  
그런데 "왜 열은 높은 곳에서 낮은 곳으로 이동하나?"  
"카르노 엔진의 예를 들자면 아무리 이상적인 열기관을 만들더라도 열효율이 결코 100%가 되지 않으며 어떻게든 낭비되는 열량이 있다."  
"즉, 가만히 있던 분자가 어떤 한곳으로 이동해 한 물체를 밀어내는 일은 없으며 수면의 물이 갑자기 한곳이 솟아 오르는 경우는 없다. 고로 열은 항상 감소 하는 방향으로 움직인다."  

루돌프 클라우지우스  
그렇다면 "열은 높은 곳에서 낮은 곳으로 이동하나 에너지 보존 법칙에 의해 열은 일로 전환된 만큼 소모되어 줄어든다. 이를 엔트로피라 하자!!"  
예> Th: 뜨거운 돌의 온도, Tc: 찬물의 온도  
돌을 물에 떨어 뜨리면 엔트로피는?  
S = Q{(1/Tc) - (1/Th)}  
Th>Tc 므로 엔트로피 s>0  
즉, 엔트로피는 항상 증가하는 방향으로 이동한다.  

루트비히 볼츠만  
"너희들의 주장에 따르면 기체 평균 분자 운동 에너지는 온도에 비례 하는군"  
"하지만 이는 거시적 상태(눈에 보이는)에서의 이상적 논리이고 미시적 상태(눈에 보이지 않는)에는 분자는 무질서하게 운동하므로 충돌 등과 같이 보다 복잡한 에너지 관계를 갖는다."  
"즉, 미시적 상태의 통계적 결과가 거시적 상태가 되는 것이다."  
하지만 당시 원자론은 증명되지 못 했고 확률통계는 신을 모독하는 죄로 해당 이론은 무시당함 그리고 자살  
추후 아인슈타인이 원자론 증명 그래서 죽은 후에야 인정 받은 이론   

# S = klnW (볼츠만 엔트로피)  
S: 엔트로피  
k: 볼츠만 상수  
W: 미시적 상태에 원자가 가질수 있는 배열의 수  
증명>  
N: 에너지를 가진 원자수  
a~k개의 뽑는 경우가 있다 가정  
W = {N!/((Na!)x(N-Na)!)} x {(N-Na)!/((Nb!)x(N-Na-Nb)!)} x ....... x {(N-...-Nl)!/((Nk!)x(N-...-Nl-Nk)!)} -> 순열  
W = {N!/((N!Na!...Nk!)x(N-Na-....-Nk)!)}  
N-Na-Nb-.....-Nk = 0 -> 0! = 1  
W = N!product(1/Nk!)  
S = klnW에 대입  
S = kln(N!product(1/Nk!)) = (k){ln(N!) - sigma(ln(Nk))}  

# 클로드 섀넌
"그럼, 볼츠만의 이론으로 정보량의 표현이 가능하겠네"  
How? -> bit 개념 도입 (0과 1로 표현한다.)  
"정보이론에 있어서 엔트로피란 무질서한 정보량을 표현하기 위해 사용되는 최소한의 자원량 이다."  
예를 들어 보자  
연인과의 카톡을 할떄 대화 정보를 표현해야된다. 가정하자  
하트와 욕설이 있다하면 어느 것을 더 많이 쓰겠는가? (일반적으로)  
당연히 하트가 많을것이다. 그렇다면 하트를 1bit로 표현하고 욕설을 4bit로 표현하면 정보를 표현하는데 있어서 효율적이다.  
즉, 발생 빈도수가 높은 것을 낮은 bit를 사용하고 낮은 빈도수를 높은 bit를 사용  
때문에 y=-logx 그래프를 가질것 이다.  
좀 더 직관적으로 해석해보자  
2^k = W -> (k는 W 경우의 수를 표현하기 위한 최소한의 자원량)  
k = -lnW -> (k > 0 이며 W = 0 ~ 1)  

# k = -lnW  
여기서 k는 확률값으로 자연에 존재하는 사전은 무수히 발생되므로 기대값으로 구한다.  
"기대값은 각 원소 값에 각 확률을 곱한 값의 합"  
왜 기대값을 구해야 하나?  
주사위를 한번 던진 결과를 가지고 그 결과를 주사위가 준 일반적인 결과라 할 수 없다.  
어쩌다 처음에 숫자가 높을 수도 있고, 때로는 낮을 수도 있기 때문이다.  
따라서 여러번 시행하여 그에 대한 평균으로 비교해야 하기 때문에 기대값이 활용된다.  
# E[k] = -sigma(p(x)lnp(x)) 


# Cross Entropy
우리는 실제 데이터의 분포 q(x)를 모른다. 이때 모델링(딥러닝, 머신러닝,...)으로 예측한 분포 p(x)를 통해 q(x)를 구해야된다.  
이때 쓰이는 것이 loss function 중 하나인 Cross Entropy  
# E[p,q] = -sigma(q(x)lnp(x))
왜 저  공식인가? -> Cross Entropy를 최소화 하는 것은 log likelihood를 최대화 하는 것과 같다.  
확률(Probability) : 어떤 시행에서 특정 결과(sample)가 나올 가능성. 즉, 시행 전 모든 경우의 수의 가능성은 정해져 있으며 그 총합은 1(100%)이다.  
우도(가능도, Likelihood) : 어떤 시행을 충분히 수행한 뒤 그 결과(sample)를 토대로 경우의 수의 가능성을 도출하는 것  
예를 들어 보자 주사위 한개를 던저서 1이 나올 확률은 1/6이다. (즉 모델과 추정치로 부터 데이터를 구하는) 
하지만 실제로 던지게 되면 1/6로 1이 나온다는 보장은 없다. 계속 시행해서 1/6확률이 나오는 분포를 찾아야된다. 이것이 likelihood이다. (즉 데이터로 부터 정확한 모델과 추정치를 찾는)  
그렇다면 왜 Maximum Likelihood Estimation을 사용하는가? 데이터를 잘 설명하는 것이 likelihood가 높다.  
그렇다면 왜 Cross Entropy를 최소화 하는 것은 log likelihood를 최대화 하는 것인가?  
likelihood의 공식은 p(x|y) = product(p(x|y))  
최대값을 구하기 위해 편미분을 해서 0이 되는 y값을 찾자 -> d/dy{p(x|y)} = 0  
편미분을 할려고하니 product는 곱집합이라 계산이 어렵다. 그래서 로그와 음수를 취해서 최소값을 찾자  
-ln(p(x|y)) = -ln(product(p(x|y)) = -sigma(ln(p(x|y)))  
d/dy{-sigma(ln(p(x|y)))} = 0 -> 이제 이렇게 우도 해보면 Cross Entropy와 동일하다.  
즉, log likelihood를 최대화는 likelihood에 로그과 음수를 취해 최소를 구하는 것이고 이는 Cross Entropy 최소 구하는 것과 동일하다.

# 베이즈 정리
토마스 베이즈에 의해 최초로 서술된 정리  
간단한 게임을 해보자  
당신은 게임에 참가했고 해당 게임은 3개의 문이 있다. 문 뒤에는 차가 한대 염소가 두마리 있다. 쇼 진행자는 하나의 문을 선택하라 하고 문 뒤에 것을 주겠다고 한다.  
단 쇼 진행자는 무엇이 어디에 있는다 알고 있다. 자 이때 당신은 하나를 선택했고 쇼 진행자는 염소가 있는 문을 먼저 하나 보여줬다. 그리고 당신에게 다시 질문한다.  
기회를 한번 더 줄테니 선택을 바꿀텐가?  
그럼 당신은 선택을 바꾸는게 유리 할까 안 바꾸는게 유리할까?  
sol>  
정답은 바꾸는게 유리하다. 왜? 











