# AI_theory

# Entropy
사디 카르노  
산업혁명 시대 증기기관 발명  
"열은 일을 할 수 있다."  
"열은 일로 전환되어 일을 한 뒤 완전히 보존되어 열로 다시 나온다." (칼로릭 보존)  
"열은 높은 곳에서 낮은 곳으로 이동한다."  

제임스 줄  
아니다. "에너지 보존 법칙에 의해 열은 일로 전환된 만큼 소모되어 줄어든다."  
  
윌리엄 톰슨  
그런데 "왜 열은 높은 곳에서 낮은 곳으로 이동하나?"  
"카르노 엔진의 예를 들자면 아무리 이상적인 열기관을 만들더라도 열효율이 결코 100%가 되지 않으며 어떻게든 낭비되는 열량이 있다."  
"즉, 가만히 있던 분자가 어떤 한곳으로 이동해 한 물체를 밀어내는 일은 없으며 수면의 물이 갑자기 한곳이 솟아 오르는 경우는 없다. 고로 열은 항상 감소 하는 방향으로 움직인다."  

루돌프 클라우지우스  
그렇다면 "열은 높은 곳에서 낮은 곳으로 이동하나 에너지 보존 법칙에 의해 열은 일로 전환된 만큼 소모되어 줄어든다. 이를 엔트로피라 하자!!"  
예> Th: 뜨거운 돌의 온도, Tc: 찬물의 온도  
돌을 물에 떨어 뜨리면 엔트로피는?  
S = Q{(1/Tc) - (1/Th)}  
Th>Tc 므로 엔트로피 s>0  
즉, 엔트로피는 항상 증가하는 방향으로 이동한다.  

루트비히 볼츠만  
"너희들의 주장에 따르면 기체 평균 분자 운동 에너지는 온도에 비례 하는군"  
"하지만 이는 거시적 상태(눈에 보이는)에서의 이상적 논리이고 미시적 상태(눈에 보이지 않는)에는 분자는 무질서하게 운동하므로 충돌 등과 같이 보다 복잡한 에너지 관계를 갖는다."  
"즉, 미시적 상태의 통계적 결과가 거시적 상태가 되는 것이다."  
하지만 당시 원자론은 증명되지 못 했고 확률통계는 신을 모독하는 죄로 해당 이론은 무시당함 그리고 자살  
추후 아인슈타인이 원자론 증명 그래서 죽은 후에야 인정 받은 이론   

# S = klnW (볼츠만 엔트로피)  
S: 엔트로피  
k: 볼츠만 상수  
W: 미시적 상태에 원자가 가질수 있는 배열의 수  
증명>  
N: 에너지를 가진 원자수  
a~k개의 뽑는 경우가 있다 가정  
W = {N!/((Na!)x(N-Na)!)} x {(N-Na)!/((Nb!)x(N-Na-Nb)!)} x ....... x {(N-...-Nl)!/((Nk!)x(N-...-Nl-Nk)!)} -> 순열  
W = {N!/((N!Na!...Nk!)x(N-Na-....-Nk)!)}  
N-Na-Nb-.....-Nk = 0 -> 0! = 1  
W = N!product(1/Nk!)  
S = klnW에 대입  
S = kln(N!product(1/Nk!)) = (k){ln(N!) - sigma(ln(Nk))}  

# 클로드 섀넌
"그럼, 볼츠만의 이론으로 정보량의 표현이 가능하겠네"  
How? -> bit 개념 도입 (0과 1로 표현한다.)  
"정보이론에 있어서 엔트로피란 무질서한 정보량을 표현하기 위해 사용되는 최소한의 자원량 이다."  
예를 들어 보자  
연인과의 카톡을 할떄 대화 정보를 표현해야된다. 가정하자  
하트와 욕설이 있다하면 어느 것을 더 많이 쓰겠는가? (일반적으로)  
당연히 하트가 많을것이다. 그렇다면 하트를 1bit로 표현하고 욕설을 4bit로 표현하면 정보를 표현하는데 있어서 효율적이다.  
즉, 발생 빈도수가 높은 것을 낮은 bit를 사용하고 낮은 빈도수를 높은 bit를 사용  
때문에 y=-logx 그래프를 가질것 이다.  
좀 더 직관적으로 해석해보자  
2^k = W -> (k는 W 경우의 수를 표현하기 위한 최소한의 자원량)  
k = -lnW -> (k > 0 이며 W = 0 ~ 1)  

# k = -lnW  
여기서 k는 확률값으로 자연에 존재하는 사전은 무수히 발생되므로 기대값으로 구한다.  
"기대값은 각 원소 값에 각 확률을 곱한 값의 합"  
왜 기대값을 구해야 하나?  
주사위를 한번 던진 결과를 가지고 그 결과를 주사위가 준 일반적인 결과라 할 수 없다.  
어쩌다 처음에 숫자가 높을 수도 있고, 때로는 낮을 수도 있기 때문이다.  
따라서 여러번 시행하여 그에 대한 평균으로 비교해야 하기 때문에 기대값이 활용된다.  
# E[k] = -sigma(q(x)lnq(x)) 


# Cross Entropy
우리는 실제 데이터의 분포 q(x)를 모른다. 이때 모델링(딥러닝, 머신러닝,...)으로 예측한 분포 p(x)를 통해 q(x)를 구해야된다.  
이때 쓰이는 것이 loss function 중 하나인 Cross Entropy  
# E[p,q] = -sigma(q(x)lnp(x))
왜 저  공식인가? -> Cross Entropy를 최소화 하는 것은 log likelihood를 최대화 하는 것과 같다.  
확률(Probability) : 어떤 시행에서 특정 결과(sample)가 나올 가능성. 즉, 시행 전 모든 경우의 수의 가능성은 정해져 있으며 그 총합은 1(100%)이다.  
우도(가능도, Likelihood) : 어떤 시행을 충분히 수행한 뒤 그 결과(sample)를 토대로 경우의 수의 가능성을 도출하는 것  
예를 들어 보자 주사위 한개를 던저서 1이 나올 확률은 1/6이다. (즉 모델과 추정치로 부터 데이터를 구하는) 
하지만 실제로 던지게 되면 1/6로 1이 나온다는 보장은 없다. 계속 시행해서 1/6확률이 나오는 분포를 찾아야된다. 이것이 likelihood이다. (즉 데이터로 부터 정확한 모델과 추정치를 찾는)  
그렇다면 왜 Maximum Likelihood Estimation을 사용하는가? 데이터를 잘 설명하는 것이 likelihood가 높다.  
그렇다면 왜 Cross Entropy를 최소화 하는 것은 log likelihood를 최대화 하는 것인가?  
likelihood의 공식은 p(x|y) = product(p(x|y))  
최대값을 구하기 위해 편미분을 해서 0이 되는 y값을 찾자 -> d/dy{p(x|y)} = 0  
편미분을 할려고하니 product는 곱집합이라 계산이 어렵다. 그래서 로그와 음수를 취해서 최소값을 찾자  
-ln(p(x|y)) = -ln(product(p(x|y)) = -sigma(ln(p(x|y)))  
d/dy{-sigma(ln(p(x|y)))} = 0 -> 이제 이렇게 유도 해보면 Cross Entropy와 동일하다.  
즉, log likelihood를 최대화는 likelihood에 로그과 음수를 취해 최소를 구하는 것이고 이는 Cross Entropy 최소 구하는 것과 동일하다.

# 베이즈 정리
"토마스 베이즈에 의해 최초로 서술된 정리"  
P(A|B) = {{P(B|A)P(A)} / P(B)}  
"어떤 사건이 만들어 놓은 상황에서, 그 사건이 일어난 후 앞으로 일어나게 될 다른 사건의 가능성을 구하는 것"  
셜록홈즈의 명대사 "설령 믿어지지 않는다고 해도, 가능성을 제외하고 남는 게 진짜 범인이다"  
Q> 간단한 게임을 해보자  
당신은 게임에 참가했고 해당 게임은 3개의 문이 있다. 문 뒤에는 차가 한대 염소가 두마리 있다. 쇼 진행자는 하나의 문을 선택하라 하고 문 뒤에 것을 주겠다고 한다.  
단 쇼 진행자는 무엇이 어디에 있는다 알고 있다. 자 이때 당신은 하나를 선택했고 쇼 진행자는 염소가 있는 문을 먼저 하나 보여줬다. 그리고 당신에게 다시 질문한다.  
기회를 한번 더 줄테니 선택을 바꿀텐가?  
그럼 당신은 선택을 바꾸는게 유리 할까 안 바꾸는게 유리할까?  
sol>  
정답은 바꾸는게 유리하다. 왜?  
처음 당신은 아무것도 모르는 상태에서 선택하여 차를 얻을 확률은 33.3%이다. 하지만 쇼 진행자가 염소를 하나 보여 주였다. 그럼 이제 변수는 달라진다. 하나가 배제 되었으니 66.6%가 되는 것이다. 그러니 당연히 저에게 33.3%의 확률을 더 주셔서 감사합니다. 하고 바꾸어야 된다.  
이를 베이즈 정리와 같이 적용해보자.  
당신이 1번 문을 선택한 경우 진행자가 3번문을 열었을 때 자동차가 1번 문에 있을 확률 : 1/3 -> P(car1|select3) : 조건부 확률  
P(car1|select3) = P(select3|car1)P(car1) / P(select3) = 1/3  
P(select3|car1) = 1/2  
P(car1) = 1/3  
P(select3) = 1/2  
그럼 이제 3번 문을 열었습니다. 이때 자동차가 2번 문에 있을 확률 : 2/3 -> P(car2|select3) : 조건부 확률  
P(car2|select3) = P(select3|car2)P(car2) / P(select3) = 2/3  
P(select3|car2) = 1 -> 당신이 1번을 선택했기 때문에 2번에 차가 있으면 결국 3번 밖에 열수없다  
P(car2) = 1/3  
P(select3) = 1/2  
이는 AI의 추론에 큰 도움이 된다. 데이터가 많으면 많을수록 올바른 추론에 도달한다. 즉 사전 확률을 지속적 업데이트 하면은 성능은 올라간다.  
Q> 넷플릭스는 돈을 어떻게 벌까요?  
바로 추천 시스템 -> 다양한 알고리즘이 있지만 가장 기본인 naive bayes  
처음 가압한 사람은 무엇을 좋아하는지 넷플리스는 알지 못 한다. 이를 cold start라 한다. 때문에 무엇을 보든 그것을 좋아할 확률을 50% 싫어할 확률도 50%으로 가정한다.  
여기서 해당 콘텐츠를 좋아할 확률 50%를 사전 확률이라 한다. 시간이 지나고 총 과련 콘텐츠를 10편을 보았다.  
예를들어 관련 콘테츠를 영화라 하자 그리고 10편 중 5편을 좋아요를 눌렀고 그중 3편은 액션장르였다 즉, 좋아하는 영화 중 60%가 액션이다.  
그리고 나머지 5편은 싫어요 인데 이중 액션은 1편이 였다. 즉 싫어하는 영화 중 20%가 액션이다.  
초기에 좋고 싫은 확률을 50%(사전확률)로 두었다. 그럼 베이즈 이론에 의해 액션영화 중 좋아요를 누를 확률은 0.75 = 0.3/((0.5x0.6)+(0.5x0.2))  
이제 사전확률을 75%(사후확률)로 업데이트 할 수 있습니다. 그러니 넷플릭스는 액션영화를 더 많이 노출 시킬것입니다.  
시간이 지나 시청자는 넷플리스에게 정보를 제공했다고 가정하자  
10편의 액션영화를 더 보고 이중 좋아요는 5편인데 그중 4편에 라이언 레이놀즈가 나왔다. 좋아하는 영화에 라이언 레이놀즈가 나올 확률 80%  
싫어요 5편 중 1편만이 라이언 레이놀즈가 나왔다. 싫어하는 영화에 라이언 레이놀즈가 나올 확률 20%  
그렇다면 라이언 레이놀즈가 나온다면 그 영화를 얼마나 좋아할까? 그 확률은?  
베이즈에 따라 구하면 이미 사전에 액션영화를 좋아할 확률을 75%로 업데이트 했다. 그러므로 75:25의 비율에서 계산하자  
(0.75x0.8)/(0.75x0.8)+(0.25x0.2) = 0.6/0.65 = 0.923 -> 92.3% 그럼 이 정보를 가지고 그 사람은 라이언 레이놀즈가 나오는 액션 영화에 노출시키면 좋을것이다. 

# Kullback–Leibler divergence (KL - Divergence)
내가 모델링하여 추론한 엔트로피와 정확하게 모델링한 엔트로피 간의 오차율(cost)  
KL Divergence = cross entropy - entropy 여기서 entropy는 사실의 고정 값이므로 오차율(cost)를 최소로하는 것은 cross entropy를 최소화하는 것이다.  
KL Divergence = E[p,q] - E[k] = (-sigma(q(x)lnp(x))) - (-sigma(q(x)lnq(x))) = -sigma{q(x)ln(p(x)/q(x))} -> 예측 모델링 모양만 다르지 결국 Cross Entropy와 동일  

# Mutual Information
KL Divergence는 나의 모델링과 실제 모델링 간의 오차율(error)를 찾는 것이면 Mutual Information는 반대로 둘이 얼마나 비슷한지를 찾는 척도 이다.  


















